# -*- coding: utf-8 -*-
"""Copy of Textfooler Asafya with Sina

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18APG4t78DlKKxfhxX1CU8w3K2mI7t2Ws
"""

!pip uninstall -y numpy pandas requests pyarrow opencv-python-headless -q
!pip install -U \
  "numpy" \
  "pandas==1.5.3" \
  "requests==2.28.2" \
  "pyarrow==10.0.1" \
  "scipy==1.10.1" \
  "opencv-python-headless==4.8.0.76" \
  "transformers" \
  "datasets" \
  "evaluate" \
  "nltk==3.8.1" \
  "textattack" \
  "gensim==4.3.2" \
  "farasapy==0.0.11"

import os
os.environ["WANDB_DISABLED"] = "true"

import nltk
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("omw-1.4")

# ============================================
# ✅ Imports
# ============================================
import importlib.util
import requests
import evaluate
from farasa.pos import FarasaPOSTagger
from datasets import Dataset as HFDataset
from nltk.corpus import wordnet as wn

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

from textattack.models.wrappers import HuggingFaceModelWrapper
from textattack.attack import Attack
from textattack.attacker import Attacker
from textattack.attack_args import AttackArgs
from textattack.search_methods import GreedyWordSwapWIR
from textattack.goal_functions.classification import UntargetedClassification
from textattack.constraints.pre_transformation import RepeatModification, StopwordModification
from textattack.constraints.grammaticality import PartOfSpeech
from textattack.constraints.semantics import WordEmbeddingDistance
from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder
from textattack.constraints.pre_transformation.input_column_modification import InputColumnModification
from textattack.datasets import Dataset
from textattack.transformations.word_swaps.word_swap import WordSwap

# ============================================
# ✅ Load FastText Arabic Embeddings
# ============================================
!wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz
from gensim.models import KeyedVectors
embedding_model = KeyedVectors.load_word2vec_format("cc.ar.300.vec.gz", binary=False)

# ============================================
# ✅ Farasa POS Initialization
# ============================================
farasa_pos = FarasaPOSTagger(interactive=True)

# ============================================
# ✅ Arabic Hybrid Synonym Swap (FastText + WordNet + Sina)
# ============================================
class ArabicTextFoolerWordSwap(WordSwap):
    def __init__(self, max_candidates=50):
        super().__init__()
        self.max_candidates = max_candidates

    def fetch_sina_synonyms(self, word):
        try:
            response = requests.get(f"https://sina.birzeit.edu/synonyms/api/{word}", timeout=2)
            if response.status_code == 200:
                data = response.json()
                return [item["synonym"] for item in data if item.get("synonym")]
        except Exception:
            pass
        return []

    def get_replacement_words(self, word):
        """Generate synonym candidates from FastText, WordNet, and Sina."""
        candidates = set()

        # FastText
        try:
            neighbors = embedding_model.most_similar(word, topn=self.max_candidates)
            candidates.update([w for w, _ in neighbors])
        except KeyError:
            pass

        # Arabic WordNet
        for syn in wn.synsets(word, lang='arb'):
            for lemma in syn.lemma_names('arb'):
                if lemma != word:
                    candidates.add(lemma)

        # Sina API
        sina_syns = self.fetch_sina_synonyms(word)
        candidates.update(sina_syns)

        return list(candidates)

    def _get_transformations(self, current_text, indices_to_modify=None):
        """Returns transformed texts with modified index tracking."""
        transformed_texts = []
        indices_to_modify = indices_to_modify or range(len(current_text.words))

        for idx in indices_to_modify:
            word = current_text.words[idx]
            replacements = self.get_replacement_words(word)

            for replacement in replacements:
                if replacement == word or not replacement.strip():
                    continue

                new_text = current_text.replace_word_at_index(idx, replacement)
                new_text.attack_attrs["newly_modified_indices"] = {idx}
                transformed_texts.append(new_text)

        return transformed_texts

    def extra_repr(self):
        return f"(max_candidates={self.max_candidates})"

# ============================================
# ✅ Load Arabic Dataset
# ============================================
dataset_path = "/content/arbml_arabic_100_reviews.py"
spec = importlib.util.spec_from_file_location("arbml_dataset", dataset_path)
arbml_dataset = importlib.util.module_from_spec(spec)
spec.loader.exec_module(arbml_dataset)
data = arbml_dataset.data

texts, labels = zip(*data)
full_dataset = HFDataset.from_dict({"text": texts, "label": labels})

# ============================================
# ✅ Tokenizer and Preprocessing
# ============================================
model_name = "asafaya/bert-base-arabic"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

full_dataset = full_dataset.map(tokenize_function, batched=True)
full_dataset = full_dataset.rename_column("label", "labels")
full_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# ============================================
# ✅ Fine-Tune Arabic BERT
# ============================================
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"],
        "f1": f1_metric.compute(predictions=predictions, references=labels, average="weighted")["f1"]
    }

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

training_args = TrainingArguments(
    output_dir="./results",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=full_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

# ============================================
# ✅ Wrap Model for Attack
# ============================================
model_wrapper = HuggingFaceModelWrapper(model, tokenizer)

# ============================================
# ✅ Build Arabic TextFooler-Style Attack
# ============================================
def build_arabic_textfooler_attack(model_wrapper):
    transformation = ArabicTextFoolerWordSwap(max_candidates=50)

    constraints = [
        RepeatModification(),
        StopwordModification(),
        InputColumnModification(
            matching_column_labels=["premise", "hypothesis"],
            columns_to_ignore=["premise"]
        ),
        WordEmbeddingDistance(min_cos_sim=0.5),
        PartOfSpeech(allow_verb_noun_swap=True),

    ]

    goal_function = UntargetedClassification(model_wrapper)
    search_method = GreedyWordSwapWIR(wir_method="delete")
    return Attack(goal_function, constraints, transformation, search_method)

# ============================================
# ✅ Run the Attack
# ============================================
attack_dataset = Dataset(list(zip(texts, labels)))
attack = build_arabic_textfooler_attack(model_wrapper)

attack_args = AttackArgs(
    num_examples=len(attack_dataset),
    log_to_csv="arabic_textfooler_combined_results.csv",
    csv_coloring_style="plain",
    disable_stdout=False,
    random_seed=42,
)

attacker = Attacker(attack, attack_dataset, attack_args)
attacker.attack_dataset()